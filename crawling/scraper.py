import os
import math
import logging
from datetime import date
from dotenv import load_dotenv
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError

from .page_handler import init_browser
from .saver import save_to_csv
from .user_agent import random_user_agent
from .utils import random_sleep, periodic_rest

load_dotenv(override=True)

TARGET_URL = os.getenv("TARGET_URL")


def crawl_jobkorea_multiple_pages():
    MAX_ITEMS = 50
    SAVE_CNT = 5
    total_items = 0
    data_batch = []

    header = ["ê¸°ì—…ëª…", "ê³µê³ ëª…", "ê²½ë ¥", "í•™ë ¥", "í˜•íƒœ", "ì§€ì—­", "ì—°ë´‰", "ë§ˆê°ì¼", "ì„¤ëª…", "ë§í¬"]
    today_str = date.today().strftime("%Y-%m-%d")

    base_path = os.getenv("RECRUIT_CSV_PATH")
    dir_path = os.path.dirname(base_path)

    csv_filename = f"jobkorea_data_{today_str}.csv"
    output_csv = os.path.join(dir_path, csv_filename)

    ua = random_user_agent()
    headers = {
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.jobkorea.co.kr/",
        "DNT": "1"
    }

    playwright, browser, context, page = init_browser(ua, headers)

    def safe_wait_networkidle(page, desc="í˜ì´ì§€", retries=3, timeout=30000):
        """
        networkidle ëŒ€ê¸° ì¤‘ TimeoutError ë°œìƒ ì‹œ ì¬ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ False ë¦¬í„´
        """
        for attempt in range(1, retries + 1):
            try:
                page.wait_for_load_state("networkidle", timeout=timeout)
                return True
            except PlaywrightTimeoutError:
                logging.warning(f"{desc} ë¡œë”© ì‹œê°„ ì´ˆê³¼ ({attempt}/{retries}), ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                if attempt < retries:
                    try:
                        page.reload()
                        page.wait_for_load_state("networkidle", timeout=timeout)
                        return True
                    except PlaywrightTimeoutError:
                        continue
        return False

    try:
        page.goto(TARGET_URL)
        if not safe_wait_networkidle(page, desc="ì´ˆê¸° í˜ì´ì§€ ì´ë™"):  # ì´ˆê¸° ë¡œë“œ ì‹œë„
            logging.error("ì´ˆê¸° í˜ì´ì§€ ë¡œë“œì— ì‹¤íŒ¨í•´ í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # ë“±ë¡ì¼ìˆœ ì •ë ¬ ë° í˜ì´ì§€ë‹¹ 50ê°œ ì„¤ì •
        page.wait_for_selector("select#orderTab")
        page.select_option("select#orderTab", value="2")
        page.select_option("select#pstab", value="50")
        page.click("button#dev-gi-search")
        if not safe_wait_networkidle(page, desc="í•„í„° ì ìš© í›„ í˜ì´ì§€ ì´ë™"):  # í•„í„° ì ìš© í›„
            logging.warning("í•„í„° ì ìš© í›„ í˜ì´ì§€ ë¡œë“œì— ë¬¸ì œê°€ ë°œìƒí–ˆìœ¼ë‚˜ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

        # ì´ ê³µê³  ìˆ˜ ê³„ì‚°
        page.wait_for_selector("input#hdnGICnt", state="attached")
        total_jobs = int(
            page.query_selector("input#hdnGICnt").get_attribute("value").replace(",", "")
        )
        max_pages_estimate = math.ceil(total_jobs / MAX_ITEMS)
        logging.info(f"ğŸ”¢ ì´ ê³µê³  ìˆ˜: {total_jobs}ê±´ â†’ ìµœëŒ€ í˜ì´ì§€ ìˆ˜: {max_pages_estimate} í˜ì´ì§€")

        current_page = 1

        while True:
            logging.info(f"\nğŸ“„ í˜„ì¬ í˜ì´ì§€: {current_page}")
            page.wait_for_selector("tr.devloopArea")
            rows = page.query_selector_all("tr.devloopArea")

            stop_crawl = False
            for row in rows[:MAX_ITEMS]:
                # ë“±ë¡ ì‹œê°„ ì •ë³´ ì½ê¸°
                time_el = row.query_selector("td.odd .time")
                time_text = time_el.inner_text().strip() if time_el else ""
                # 1ì¼ ì´ë‚´: 'ë¶„ ì „ ë“±ë¡' ë˜ëŠ” 'ì‹œê°„ ì „ ë“±ë¡'ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                # title_el = row.query_selector("td.tplTit strong a")
                # title = title_el.inner_text().strip()
                # print(time_text, " : ",title)
                if "ë¶„ ì „ ë“±ë¡" in time_text or "ì‹œê°„ ì „ ë“±ë¡" in time_text:
                    try:
                        title_el = row.query_selector("td.tplTit strong a")
                        title = title_el.inner_text().strip()
                        link = f"https://www.jobkorea.co.kr{title_el.get_attribute('href')}"
                        company = row.query_selector("td.tplCo a").inner_text().strip()

                        etc_tags = row.query_selector_all("td.tplTit .etc .cell")
                        career = etc_tags[0].inner_text().strip() if len(etc_tags) > 0 else ""
                        education = etc_tags[1].inner_text().strip() if len(etc_tags) > 1 else ""
                        location = etc_tags[2].inner_text().strip() if len(etc_tags) > 2 else ""
                        emp_type = etc_tags[3].inner_text().strip() if len(etc_tags) > 3 else ""
                        salary = etc_tags[4].inner_text().strip() if len(etc_tags) > 4 else ""
                        position = etc_tags[5].inner_text().strip() if len(etc_tags) > 5 else ""

                        deadline_el = row.query_selector("td.odd .date")
                        deadline = deadline_el.inner_text().strip() if deadline_el else ""

                        description_el = row.query_selector("td.tplTit p.dsc")
                        description = description_el.inner_text().strip() if description_el else ""

                        data_batch.append([
                            company, title, career, education, emp_type,
                            location, salary, deadline, description, link
                        ])
                        total_items += 1
                        logging.info(f"{total_items:04d} | {company} | {title} | {time_text}")
                    except Exception as e:
                        logging.info(f"âš ï¸ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                else:
                    stop_crawl = True
                    break

            if stop_crawl:
                logging.info("â›” 1ì¼ ì´ë‚´ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì™„ë£Œ â€” ì¢…ë£Œ")
                break

            if current_page % SAVE_CNT == 0:
                save_to_csv(output_csv, data_batch, header)
                logging.info(f"âœ… {SAVE_CNT}í˜ì´ì§€ë§ˆë‹¤ CSV ì €ì¥ ì™„ë£Œ (ì´ {total_items}ê±´)")
                data_batch.clear()

            random_sleep()
            periodic_rest(current_page)

            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ë° ëŒ€ê¸°
            next_page = page.query_selector(f'div.tplPagination li a[data-page="{current_page + 1}"]')
            if next_page:
                next_page.click()
                if not safe_wait_networkidle(page, desc=f"{current_page+1}í˜ì´ì§€ ì´ë™"):  # í˜ì´ì§€ ì „í™˜
                    logging.warning(f"{current_page+1}í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨, ë°˜ë³µ ì¤‘ë‹¨")
                    break
                current_page += 1
            else:
                more_button = page.query_selector('a.btnPgnNext')
                if more_button and "disabled" not in (more_button.get_attribute("class") or ""):
                    more_button.click()
                    if not safe_wait_networkidle(page, desc="ë‹¤ìŒ ë¸”ë¡ ì´ë™"):  # ë¸”ë¡ ì „í™˜
                        logging.warning("ë‹¤ìŒ ë¸”ë¡ ë¡œë“œ ì‹¤íŒ¨, ë°˜ë³µ ì¤‘ë‹¨")
                        break
                    current_page += 1
                else:
                    logging.info("ğŸ”š í˜ì´ì§€ íƒìƒ‰ ì¢…ë£Œ")
                    break

        if data_batch:
            save_to_csv(output_csv, data_batch, header)
            logging.info(f"ğŸ“ ë§ˆì§€ë§‰ ë°ì´í„° ì €ì¥ ì™„ë£Œ (ì´ {total_items}ê±´)")

    finally:
        browser.close()
        playwright.stop()

def run_crawler_with_retry(max_retries=3):
    """
    TimeoutError ë“± ì˜ˆì™¸ ë°œìƒ ì‹œ ì „ì²´ í¬ë¡¤ëŸ¬ë¥¼ ìµœëŒ€ max_retriesê¹Œì§€ ì¬ì‹œë„
    """
    for attempt in range(1, max_retries + 1):
        try:
            logging.info(f"\nğŸš€ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œë„ {attempt}/{max_retries}")
            crawl_jobkorea_multiple_pages()
            logging.info("âœ… í¬ë¡¤ëŸ¬ ì •ìƒ ì¢…ë£Œ")
            break
        except Exception as e:
            logging.warning(f"âš ï¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__} - {e}")
            if attempt < max_retries:
                logging.info("ğŸ” í¬ë¡¤ëŸ¬ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            else:
                logging.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ â€” í¬ë¡¤ëŸ¬ ì¤‘ë‹¨")